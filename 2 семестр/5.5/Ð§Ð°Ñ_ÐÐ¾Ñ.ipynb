{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paDNYdzzdF7i",
        "outputId": "03e12ce9-59cf-4be4-8e59-36fc837cbf4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/andor/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/home/andor/.local/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/home/andor/.local/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
            "caused by: [\"[Errno 2] The file to load file system plugin from does not exist.: '/home/andor/.local/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so'\"]\n",
            "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
            "/home/andor/.local/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/home/andor/.local/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
            "caused by: ['/home/andor/.local/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: cannot open shared object file: No such file or directory']\n",
            "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/andor/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "# Импорт библиотек, необходимых для NLP\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# Импорт библиотек, необходимых для обработки Tensorflow\n",
        "import tensorflow as tf   \n",
        "import numpy as np\n",
        "import tflearn           \n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install tensorflow\n",
        "!pip install numpy\n",
        "!pip install tflearn\n",
        "!pip install random\n",
        "!pip install json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hn3P5ibadF7j"
      },
      "outputs": [],
      "source": [
        "# импортируем наш файл intents.json, используемый для обучения модели.\n",
        "with open(\"/home/andor/Рабочий стол/5.5/intents.json\") as json_data: \n",
        "    intents = json.load(json_data)      # Загрузка данных из файла intents.json в var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vUL1tWz3dF7j"
      },
      "outputs": [],
      "source": [
        "# Пустые списки для добавления данных после обработки NLP\n",
        "words=[]\n",
        "documents = []\n",
        "classes = []\n",
        "\n",
        "\n",
        "# Этот список будет использоваться для игнорирования всех нежелательных знаков препинания.\n",
        "ignore = [\"?\"]\n",
        "\n",
        "# Запуск цикла по каждому intents в intents[\"patterns\"]\n",
        "for intent in intents[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        \n",
        "        # токенизация каждого слова в предложении с помощью токенизатора слов и сохранение в w\n",
        "        w = nltk.word_tokenize(pattern) \n",
        "        #print(w)\n",
        "        \n",
        "        # Добавление токенизированных слов в пустой список слов, который мы создали\n",
        "        words.extend(w) \n",
        "        #print(words)\n",
        "        \n",
        "        # Добавление слов в документы с тегом, указанным в файле намерений intents\n",
        "        documents.append((w, intent[\"tag\"]))\n",
        "        #print(documents)\n",
        "        \n",
        "        # Добавление только тега в список наших классов\n",
        "        if intent[\"tag\"] not in classes:      \n",
        "            classes.append(intent[\"tag\"])  #Если тег отсутствует в классах [], он будет добавлен к нему..\n",
        "            #print(classes)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hhvqBf7dF7k",
        "outputId": "92a80892-5876-4845-b55c-92c774a48fe9",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12 Documents \n",
            "\n",
            "4 Classes \n",
            "\n",
            "32 Stemmed Words \n"
          ]
        }
      ],
      "source": [
        "#Выполнение стемминга с помощью stemmer.stem()\n",
        "#ЗАпуск повторений в  words[] и игнорирование знаков препинания, присутствующих в ignore[]\n",
        "\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]  \n",
        "words = sorted(list(set(words)))  #Removing Duplicates in words[]\n",
        "\n",
        "#Удаление повторяющихся классов\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "#Длина печати сформированных нами списков\n",
        "print(len(documents),\"Documents \\n\")\n",
        "print(len(classes),\"Classes \\n\")\n",
        "print(len(words), \"Stemmed Words \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vsqecXpxdF7k"
      },
      "outputs": [],
      "source": [
        "#Создание обучающих данных, которые в дальнейшем будут использоваться для обучения\n",
        "training = []\n",
        "output = []\n",
        "\n",
        "#Creating empty array for output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "#Создание пустого массива для вывода\n",
        "for doc in documents:\n",
        "    bag = [] #Инициализация пустого списка слов\n",
        "\n",
        "    pattern_words = doc[0] #Хранение списка токенизированных слов для документов[] tp pattern_words\n",
        "    #print(pattern_words)\n",
        "    \n",
        "    #Снова вывод каждого слова из pattern_words\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]  \n",
        "    #print(pattern_words)\n",
        "    \n",
        "    #Создание массива списка слов\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "        \n",
        "    #Это даст результат 1 для текущего тега и 0 для всех остальных тегов.\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] =1 \n",
        "    training.append([bag, output_row])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK7Vn0IJdF7k",
        "outputId": "7894a779-1624-4318-c310-5cc7547cf5df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3261/3014420881.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training = np.array(training) #Преобразование данных обучения в массив numpy array\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(training) #Перетасовка данных или функций\n",
        "training = np.array(training) #Преобразование данных обучения в массив numpy array\n",
        "\n",
        "#Создание списков тренировок\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcOc7-uIdF7k",
        "outputId": "a31ad291-5d56-4052-a384-18a09e6f75b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.35243\u001b[0m\u001b[0m | time: 0.002s\n",
            "| Adam | epoch: 1000 | loss: 0.35243 - acc: 0.9484 -- iter: 08/12\n",
            "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.31771\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1000 | loss: 0.31771 - acc: 0.9536 -- iter: 12/12\n",
            "--\n",
            "INFO:tensorflow:/home/andor/Рабочий стол/5.5/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ],
      "source": [
        "tf.compat.v1.reset_default_graph() #Сброс данных базового графика\n",
        "\n",
        "#Создание собственной нейронной сети\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "#Определение модели и настройкаp tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir=\"tflearn_logs\") \n",
        "\n",
        "#Теперь у нас есть настроенная модель, теперь нам нужно обучить эту модель, вписав в нее данные с помощью model.fit()\n",
        "#n_epoch это сколько раз модель будет использовать ваши данные во время обучения\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True) \n",
        "model.save(\"model.tflearn\") #Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EOYtMW0FdF7l"
      },
      "outputs": [],
      "source": [
        "#Импорт модуля pickle\n",
        "import pickle\n",
        "\n",
        "#Сброс обучающих данных с помощью dump() и запись их в training_data в двоичном режиме\n",
        "pickle.dump({\"words\":words, \"classes\":classes, \"train_x\":train_x, \"train_y\":train_y}, open(\"training_data\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5LJbxp7CdF7l"
      },
      "outputs": [],
      "source": [
        "#Восстановление всей структуры данных\n",
        "data = pickle.load(open(\"training_data\",\"rb\"))\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-weKS5IPdF7l"
      },
      "outputs": [],
      "source": [
        "with open(\"/home/andor/Рабочий стол/5.5/intents.json\") as json_data:\n",
        "    intents = json.load(json_data)  #Загрузка наших json_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Nq7DvxBddF7l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /home/andor/Рабочий стол/5.5/model.tflearn\n"
          ]
        }
      ],
      "source": [
        "# Загрузка сохраненной модели\n",
        "model.load(\"./model.tflearn\") #Загрузка модели обучения, которую мы сохранили"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-NJEDLn8dF7l"
      },
      "outputs": [],
      "source": [
        "#Очистка пользовательского ввода\n",
        "def clean_up_sentence(sentence):\n",
        "    \n",
        "    # Токенизация шаблона\n",
        "    sentence_words = nltk.word_tokenize(sentence) #Again tokenizing the sentence\n",
        "    \n",
        "    #Получение каждого слова из ввода пользователя\n",
        "    sentence_words= [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "\n",
        "    return sentence_words\n",
        "\n",
        "#Возвращаемый массив массива слов: 0 или 1 или каждое слово в списке, которое существует, как мы объявили в приведенных выше строках.\n",
        "def bow(sentence, words, show_details=False):\n",
        "    \n",
        "    #Токенизация пользовательского ввода\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    \n",
        "    #Генерация набора слов из предложения, которое ввел пользователь\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print(\"Found in bag: %s\"% w)\n",
        "    return(np.array(bag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OGy6_JnKdF7m"
      },
      "outputs": [],
      "source": [
        "#Добавление некоторого контекста в разговор для лучших результатов..\n",
        "\n",
        "context = {} #Создайте словарь для хранения контекста пользователя\n",
        "\n",
        "ERROR_THRESHOLD = 0.25\n",
        "def classify(sentence):\n",
        "    \n",
        "    #Генерация вероятностей из модели\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    \n",
        "    #Отфильтровать прогнозы ниже порогового значения\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    \n",
        "    #Сортировка по силе вероятности\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    \n",
        "    # вернуть кортеж намерения и вероятности\n",
        "    return return_list\n",
        "\n",
        "def query(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    \n",
        "    #Если у нас есть классификация, найдите соответствующий тег намерения\n",
        "    if results:\n",
        "        \n",
        "        #Цикл, пока есть совпадения для обработки\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                \n",
        "                #Найти тег, соответствующий первому результату\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    \n",
        "                    #Установить контекст для этого намерения, если необходимо\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print ('context:', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "\n",
        "                    # проверить, является ли это намерение контекстуальным и применимо к разговору этого пользователя\n",
        "                    if not 'context_filter' in i or \\\n",
        "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details: print ('tag:', i['tag'])\n",
        "                        \n",
        "                        #Случайный ответ от намерения\n",
        "                        return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBgLJHkqdF7m",
        "outputId": "505e6db6-4be6-4a6c-ec65-59f431774de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Авилон приветствует вас !\n"
          ]
        }
      ],
      "source": [
        "query(\"Здравствуйте\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJmiQR_Vq_51",
        "outputId": "620e838b-a35e-4668-a5ca-3a20e71fcac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Вам нужно разрешение директора автосалона\n"
          ]
        }
      ],
      "source": [
        "query(\"Вы не против, если мы будем снимать ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mKnnsJ6rJel",
        "outputId": "9436ec2b-7647-49aa-b521-95aa935d1a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Я же вам сказал, снимать нельзя\n"
          ]
        }
      ],
      "source": [
        "query (\"Автосалон это общественное место, а значит мы можем снимать\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEessc_5tNQk",
        "outputId": "e610996d-51ea-4759-8c38-232dc256e7c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Возращайтесь без камер.\n"
          ]
        }
      ],
      "source": [
        "query (\"Ладно, мы уходим\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
